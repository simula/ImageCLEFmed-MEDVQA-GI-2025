{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twcNgCtfiwju"
      },
      "source": [
        "\n",
        "# üöÄ **Fine-Tuning Model with ms-swift for ImageCLEFmed-MEDVQA-GI-2025's Subtask 1**\n",
        "\n",
        "üîç Subtask 1: Algorithm Development for Question Interpretation and Response\n",
        "\n",
        "This notebook demonstrates the **fine-tuning process** for a model (deepseek-ai/Janus-Pro-1B) on the **Kvasir-VQA dataset** using **[ms-swift](https://swift.readthedocs.io/en/latest/GetStarted/Quick-start.html)** package, which contains **medical images üè•** with related **questions ‚ùì** and **answers ‚úÖ**.  You can use **ms-swift** to experiment with a lot of models, parameters and strategies with ease.\n",
        "\n",
        "üîó **Competition Repository:** [ImageCLEFmed-MEDVQA-GI-2025](https://github.com/simula/ImageCLEFmed-MEDVQA-GI-2025) üåê  \n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Steps Included**:  \n",
        "- ‚öôÔ∏è **Environment setup** and installation of requirements üì¶  \n",
        "- üß© **Dataset prep** üóÇÔ∏è  \n",
        "- üßπ **Data preprocessing** üßº  \n",
        "- ü§ñ **Model fine-tuning** using Hugging Face's `Trainer` API üèÉ‚Äç‚ôÇÔ∏è   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Vz1sfKiwjv"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!!pip install datasets ms-swift flash_attn deepspeed mpi4py\n",
        "\n",
        "# Import necessary modules\n",
        "from datasets import load_dataset\n",
        "import json, os\n",
        "\n",
        "# comment this (RECOMMENDED) if you want to log the run to Weights & Biases\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\" #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXWsGPX3iwjw"
      },
      "source": [
        "## Load Dataset and Prepare dataset for VLM finetuning format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_ = load_dataset(\"SimulaMet-HOST/Kvasir-VQA\")['raw']"
      ],
      "metadata": {
        "id": "2r7kESSFHd3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz1ccsC0iwjw"
      },
      "outputs": [],
      "source": [
        "img_path=\"images\"\n",
        "os.makedirs(img_path,exist_ok=True)\n",
        "existing_files=set(os.listdir(img_path))\n",
        "\n",
        "## lets prepare JSON for each row\n",
        "ds = ds_.map(lambda e: {\"msg\":(\n",
        "        e['image'].save(f\"{img_path}/{e['img_id']}.jpg\")\n",
        "        or existing_files.add(f\"{e['img_id']}.jpg\")\n",
        "        if f\"{e['img_id']}.jpg\" not in existing_files else None\n",
        "    ) or {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": f\"<image> {e['question']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": e['answer']}\n",
        "        ],\n",
        "        \"images\": [f\"{img_path}/{e['img_id']}.jpg\"]\n",
        "    }})\n",
        "ds[0]['msg'] ## lets see what is in each row"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qx6MHOkMHlK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3mD4plMiwjx"
      },
      "outputs": [],
      "source": [
        "# turned out some entries are null, so lets filter them\n",
        "jsonl_data = [entry for entry in ds['msg'] if entry['messages'][1][\"content\"]!='nan']\n",
        "# for faster DEMO, lets select only 10% of total dataset and save\n",
        "import random; jsonl_data = random.sample(jsonl_data, int(len(jsonl_data) * 0.10))\n",
        "\n",
        "open(\"kvasir-vqa.jsonl\",\"w\",encoding=\"utf-8\").writelines(json.dumps(entry)+\"\\n\" for entry in jsonl_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwJB2h-iwjx"
      },
      "source": [
        "## Train the model using swift sft CLI\n",
        "\n",
        "Define training arguments for model optimization.\n",
        "For all possible arguments and options see: https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html\n",
        "\n",
        "For use a different VLM models grab HF Model ID from the list of all supported models:\n",
        "https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html#multimodal-large-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSopcYl9iwjx"
      },
      "outputs": [],
      "source": [
        "!swift sft \\\n",
        "    --model \"deepseek-ai/Janus-Pro-1B\" \\\n",
        "    --dataset \"kvasir-vqa.jsonl\" \\\n",
        "    --use_hf true \\\n",
        "    --train_type lora \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --split_dataset_ratio 0.05 \\\n",
        "    --torch_dtype float16 \\\n",
        "    --lora_rank 4 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --target_modules all-linear \\\n",
        "    --freeze_vit true \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --eval_steps 200 \\\n",
        "    --save_steps 500 \\\n",
        "    --save_total_limit 2 \\\n",
        "    --logging_steps 1 \\\n",
        "    --max_length 1024 \\\n",
        "    --output_dir output \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --attn_impl eager \\\n",
        "    --gradient_checkpointing true \\\n",
        "    --dataloader_num_workers 1 \\\n",
        "    --deepspeed zero2_offload"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Dependingn upon your GPU config, you neeed to tune the parameters for optimal performance. The current parameters are optimised for Colab's Free T4 GPU.\n",
        "If GPU is Amere and above use 'sdpa' or 'flash_attn' for **--attn_impl**\"."
      ],
      "metadata": {
        "id": "lbl2APTpjnDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Good luck!"
      ],
      "metadata": {
        "id": "jMP2cy9emnBF"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}